{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLghoSjJ1xhf"
      },
      "source": [
        "# Install libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-jqprc70uHE"
      },
      "source": [
        "%%capture\n",
        "# Transformers installation\n",
        "! pip install transformers\n",
        "! pip install datasets\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hu70Ng0Eqls"
      },
      "source": [
        "# Getting the Data and Preview it\n",
        "Below we are going to load the data and show you how to create the splits. However, we don't need to split the data manually becuase I have already created the splits and stored those files seperately which you can quickly download below:\n",
        "\n",
        "https://www.kaggle.com/praveengovi/emotions-dataset-for-nlp/code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ3SoJH3fUsq"
      },
      "source": [
        "# %%capture \n",
        "# !wget https://www.dropbox.com/s/ikkqxfdbdec3fuj/test.txt\n",
        "# !wget https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt\n",
        "# !wget https://www.dropbox.com/s/2mzialpsgf9k5l3/val.txt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4rYMiOuSLWM"
      },
      "source": [
        "# with open('train.txt') as f:\n",
        "#     print(f.read()[:1000])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_03fxufWX_G"
      },
      "source": [
        "## export the datasets as txt files\n",
        "## EXERCISE: Change this to an address\n",
        "\n",
        "# train_path = \"train.txt\"\n",
        "# test_path = \"test.txt\"\n",
        "# val_path = \"val.txt\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t23zHggkEpc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c179200-be43-4055-8e8b-cff565e1fe94"
      },
      "source": [
        "!wget https://www.dropbox.com/s/607ptdakxuh5i4s/merged_training.pkl"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-11 13:12:45--  https://www.dropbox.com/s/607ptdakxuh5i4s/merged_training.pkl\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/607ptdakxuh5i4s/merged_training.pkl [following]\n",
            "--2021-06-11 13:12:46--  https://www.dropbox.com/s/raw/607ptdakxuh5i4s/merged_training.pkl\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucaffb13495ed7881e59ab48dd7d.dl.dropboxusercontent.com/cd/0/inline/BQN6AI56GEZLtilHN3qeWdt-cFT1Urkx3CnHQda1cy3i2CUjzS7K25zKhuHMOa4exse1IIedZWaqu_kJBfQEwvIfYNFDVhWlWYRUztfKyP8Ly0dALkdCvsxawR1GTCZJd5uZaNLmTSB5o12o8JNwN0RX/file# [following]\n",
            "--2021-06-11 13:12:46--  https://ucaffb13495ed7881e59ab48dd7d.dl.dropboxusercontent.com/cd/0/inline/BQN6AI56GEZLtilHN3qeWdt-cFT1Urkx3CnHQda1cy3i2CUjzS7K25zKhuHMOa4exse1IIedZWaqu_kJBfQEwvIfYNFDVhWlWYRUztfKyP8Ly0dALkdCvsxawR1GTCZJd5uZaNLmTSB5o12o8JNwN0RX/file\n",
            "Resolving ucaffb13495ed7881e59ab48dd7d.dl.dropboxusercontent.com (ucaffb13495ed7881e59ab48dd7d.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to ucaffb13495ed7881e59ab48dd7d.dl.dropboxusercontent.com (ucaffb13495ed7881e59ab48dd7d.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BQM6OzwlenyR-b03RPH-O161sGapvtnmNheAQhd7wTPAr8pO-bnZ_OVkWhVKT5WSudWAkuy2Z9Iy2kyB7z33MPLZxCDgOtdyPvCa5tK7zlb2mrAucsGBt_SU30sL5ZOBIILhQmURrhKtQJkW5wbRXtyufjc0bIKK0EgT2iR6DRECIrQD25lMwz0DOxLji_FXqV1Arkj9w6-dmqnUH0xEvWQA__vpd9WqaddAHNk7_MIP03gY7K9rm-1V2xOGFyW00dd5ZG-1wSQEDnjYGBRiIsbOtjYb-O3A8WkskjllFPmYjjxTtvsbmrd1wE36oglqvTDzaWuK8lbSGT3ygTxKANPKpq9LNd3_Cbi0zUnuNQRLiYL383HgW_FVUzOU5TSBbUU/file [following]\n",
            "--2021-06-11 13:12:47--  https://ucaffb13495ed7881e59ab48dd7d.dl.dropboxusercontent.com/cd/0/inline2/BQM6OzwlenyR-b03RPH-O161sGapvtnmNheAQhd7wTPAr8pO-bnZ_OVkWhVKT5WSudWAkuy2Z9Iy2kyB7z33MPLZxCDgOtdyPvCa5tK7zlb2mrAucsGBt_SU30sL5ZOBIILhQmURrhKtQJkW5wbRXtyufjc0bIKK0EgT2iR6DRECIrQD25lMwz0DOxLji_FXqV1Arkj9w6-dmqnUH0xEvWQA__vpd9WqaddAHNk7_MIP03gY7K9rm-1V2xOGFyW00dd5ZG-1wSQEDnjYGBRiIsbOtjYb-O3A8WkskjllFPmYjjxTtvsbmrd1wE36oglqvTDzaWuK8lbSGT3ygTxKANPKpq9LNd3_Cbi0zUnuNQRLiYL383HgW_FVUzOU5TSBbUU/file\n",
            "Reusing existing connection to ucaffb13495ed7881e59ab48dd7d.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49991846 (48M) [application/octet-stream]\n",
            "Saving to: ‘merged_training.pkl.1’\n",
            "\n",
            "merged_training.pkl 100%[===================>]  47.68M  29.6MB/s    in 1.6s    \n",
            "\n",
            "2021-06-11 13:12:49 (29.6 MB/s) - ‘merged_training.pkl.1’ saved [49991846/49991846]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQrMSUTRF06B"
      },
      "source": [
        "import pickle\n",
        "\n",
        "## helper function\n",
        "def load_from_pickle(directory):\n",
        "    return pickle.load(open(directory,\"rb\"))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGz89mNSHaYM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "98adf7f5-c333-4098-b900-80f33d2d1084"
      },
      "source": [
        "data = load_from_pickle(directory=\"merged_training.pkl\")\n",
        "\n",
        "## emotion labels\n",
        "label2int = {\n",
        "  \"sadness\": 0,\n",
        "  \"joy\": 1,\n",
        "  \"love\": 2,\n",
        "  \"anger\": 3,\n",
        "  \"fear\": 4,\n",
        "  \"surprise\": 5\n",
        "}\n",
        "\n",
        "data = data[data[\"emotions\"].isin(label2int.keys())]\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "data = data.sample(n=10000);\n",
        "\n",
        "data.emotions.value_counts().plot.bar()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(416809, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6deeca4650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEXCAYAAABBFpRtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZD0lEQVR4nO3dfbRddX3n8ffH8CDWB0K5ZcUkGNSog0+BXgMz0KlCgQAq+ERhKkRKm44D9WGcLoOjQ0WYRWsra9mljCARcCyY+piBAEaKY9XyEGgMhIfm8rRIBiE1CCgFhH7mj/27eAw3uTe5+56dnN/ntdZZd5/f3ufs7yacz9nnt397b9kmIiLq8LyuC4iIiP5J6EdEVCShHxFRkYR+RERFEvoRERVJ6EdEVGSnrgvYkj333NNz5szpuoyIiB3KTTfd9C+2h8aat12H/pw5c1i5cmXXZURE7FAk3be5eeN270h6vqQbJP1Y0hpJnyztF0m6R9Kq8phX2iXps5JGJK2WtH/Pey2UtLY8FraxcRERMXET2dN/EjjE9s8l7Qz8QNKVZd6f2f7aJssfCcwtjwOA84ADJO0BnAEMAwZukrTM9sNtbEhERIxv3D19N35enu5cHlu6dsMxwCXlddcBu0uaARwBrLC9sQT9CmDB5MqPiIitMaHRO5KmSVoFPEQT3NeXWWeXLpxzJe1a2mYC9/e8fF1p21z7putaJGmlpJUbNmzYys2JiIgtmVDo237G9jxgFjBf0uuA04HXAG8C9gA+2kZBts+3PWx7eGhozIPPERGxjbZqnL7tnwHXAgtsP1C6cJ4EvgTML4utB2b3vGxWadtce0RE9MlERu8MSdq9TO8GHAbcUfrpkSTgWODW8pJlwEllFM+BwCO2HwCuBg6XNF3SdODw0hYREX0ykdE7M4CLJU2j+ZJYavtySX8vaQgQsAr4z2X55cBRwAjwOHAygO2Nkj4F3FiWO9P2xvY2JSIixqPt+SYqw8PD3paTs+YsvmIKqtm8e885uq/ri4jYEkk32R4ea16uvRMRUZGEfkRERRL6EREVSehHRFQkoR8RUZGEfkRERRL6EREVSehHRFQkoR8RUZGEfkRERRL6EREVSehHRFQkoR8RUZGEfkRERRL6EREVSehHRFQkoR8RUZGEfkRERRL6EREVSehHRFQkoR8RUZFxQ1/S8yXdIOnHktZI+mRp30fS9ZJGJH1V0i6lfdfyfKTMn9PzXqeX9jslHTFVGxUREWObyJ7+k8Ahtt8IzAMWSDoQ+AvgXNuvBB4GTinLnwI8XNrPLcshaV/geOC1wALg85KmtbkxERGxZeOGvhs/L093Lg8DhwBfK+0XA8eW6WPKc8r8QyWptF9m+0nb9wAjwPxWtiIiIiZkQn36kqZJWgU8BKwA7gJ+Zvvpssg6YGaZngncD1DmPwL8Zm/7GK+JiIg+mFDo237G9jxgFs3e+WumqiBJiyStlLRyw4YNU7WaiIgqbdXoHds/A64F/j2wu6SdyqxZwPoyvR6YDVDmvwT4aW/7GK/pXcf5todtDw8NDW1NeRERMY6JjN4ZkrR7md4NOAy4nSb8310WWwh8u0wvK88p8//etkv78WV0zz7AXOCGtjYkIiLGt9P4izADuLiMtHkesNT25ZJuAy6TdBbwT8CFZfkLgS9LGgE20ozYwfYaSUuB24CngVNtP9Pu5kRExJaMG/q2VwP7jdF+N2OMvrH9BPCezbzX2cDZW19mRES0YSJ7+rGdmbP4ir6u795zju7r+iJi6uQyDBERFUnoR0RUJKEfEVGRhH5EREUS+hERFUnoR0RUJKEfEVGRhH5EREUS+hERFUnoR0RUJKEfEVGRhH5EREUS+hERFUnoR0RUJKEfEVGRhH5EREUS+hERFUnoR0RUJKEfEVGRhH5EREUS+hERFRk39CXNlnStpNskrZH0wdL+55LWS1pVHkf1vOZ0SSOS7pR0RE/7gtI2Imnx1GxSRERszk4TWOZp4CO2b5b0IuAmSSvKvHNt/1XvwpL2BY4HXgu8FPiupFeV2Z8DDgPWATdKWmb7tjY2JCIixjdu6Nt+AHigTD8m6XZg5hZecgxwme0ngXskjQDzy7wR23cDSLqsLJvQj4jok63q05c0B9gPuL40nSZptaQlkqaXtpnA/T0vW1faNte+6ToWSVopaeWGDRu2pryIiBjHhENf0guBrwMfsv0ocB7wCmAezS+Bv26jINvn2x62PTw0NNTGW0ZERDGRPn0k7UwT+F+x/Q0A2w/2zL8AuLw8XQ/M7nn5rNLGFtojIqIPJjJ6R8CFwO22P9PTPqNnsXcAt5bpZcDxknaVtA8wF7gBuBGYK2kfSbvQHOxd1s5mRETERExkT/8g4ETgFkmrStvHgBMkzQMM3Av8CYDtNZKW0hygfRo41fYzAJJOA64GpgFLbK9pcVsiImIcExm98wNAY8xavoXXnA2cPUb78i29LiIiplbOyI2IqEhCPyKiIgn9iIiKJPQjIiqS0I+IqEhCPyKiIgn9iIiKJPQjIiqS0I+IqEhCPyKiIgn9iIiKJPQjIiqS0I+IqEhCPyKiIgn9iIiKJPQjIiqS0I+IqEhCPyKiIgn9iIiKJPQjIiqS0I+IqMi4oS9ptqRrJd0maY2kD5b2PSStkLS2/J1e2iXps5JGJK2WtH/Pey0sy6+VtHDqNisiIsYykT39p4GP2N4XOBA4VdK+wGLgGttzgWvKc4AjgbnlsQg4D5ovCeAM4ABgPnDG6BdFRET0x7ihb/sB2zeX6ceA24GZwDHAxWWxi4Fjy/QxwCVuXAfsLmkGcASwwvZG2w8DK4AFrW5NRERs0Vb16UuaA+wHXA/sZfuBMusnwF5leiZwf8/L1pW2zbVvuo5FklZKWrlhw4atKS8iIsYx4dCX9ELg68CHbD/aO8+2AbdRkO3zbQ/bHh4aGmrjLSMiophQ6EvamSbwv2L7G6X5wdJtQ/n7UGlfD8zuefms0ra59oiI6JOJjN4RcCFwu+3P9MxaBoyOwFkIfLun/aQyiudA4JHSDXQ1cLik6eUA7uGlLSIi+mSnCSxzEHAicIukVaXtY8A5wFJJpwD3AceVecuBo4AR4HHgZADbGyV9CrixLHem7Y2tbEVEREzIuKFv+weANjP70DGWN3DqZt5rCbBkawqMiIj25IzciIiKJPQjIiqS0I+IqEhCPyKiIgn9iIiKJPQjIiqS0I+IqEhCPyKiIgn9iIiKJPQjIiqS0I+IqEhCPyKiIgn9iIiKJPQjIiqS0I+IqEhCPyKiIgn9iIiKJPQjIiqS0I+IqEhCPyKiIgn9iIiKjBv6kpZIekjSrT1tfy5pvaRV5XFUz7zTJY1IulPSET3tC0rbiKTF7W9KRESMZyJ7+hcBC8ZoP9f2vPJYDiBpX+B44LXlNZ+XNE3SNOBzwJHAvsAJZdmIiOijncZbwPb3Jc2Z4PsdA1xm+0ngHkkjwPwyb8T23QCSLivL3rbVFUdExDabTJ/+aZJWl+6f6aVtJnB/zzLrStvm2p9D0iJJKyWt3LBhwyTKi4iITW1r6J8HvAKYBzwA/HVbBdk+3/aw7eGhoaG23jYiIphA985YbD84Oi3pAuDy8nQ9MLtn0VmljS20R0REn2zTnr6kGT1P3wGMjuxZBhwvaVdJ+wBzgRuAG4G5kvaRtAvNwd5l2152RERsi3H39CVdCrwZ2FPSOuAM4M2S5gEG7gX+BMD2GklLaQ7QPg2cavuZ8j6nAVcD04Altte0vjUREbFFExm9c8IYzRduYfmzgbPHaF8OLN+q6iIiolU5IzcioiLbdCA3YirNWXxFX9d37zlH93V9EV3Knn5EREUS+hERFUnoR0RUJKEfEVGRhH5EREUS+hERFUnoR0RUJKEfEVGRhH5EREUS+hERFUnoR0RUJKEfEVGRhH5EREUS+hERFUnoR0RUJKEfEVGR3EQlos9yk5joUvb0IyIqktCPiKjIuKEvaYmkhyTd2tO2h6QVktaWv9NLuyR9VtKIpNWS9u95zcKy/FpJC6dmcyIiYksmsqd/EbBgk7bFwDW25wLXlOcARwJzy2MRcB40XxLAGcABwHzgjNEvioiI6J9xQ9/294GNmzQfA1xcpi8Gju1pv8SN64DdJc0AjgBW2N5o+2FgBc/9IomIiCm2rX36e9l+oEz/BNirTM8E7u9Zbl1p21x7RET00aQP5No24BZqAUDSIkkrJa3csGFDW28bERFse+g/WLptKH8fKu3rgdk9y80qbZtrfw7b59setj08NDS0jeVFRMRYtjX0lwGjI3AWAt/uaT+pjOI5EHikdANdDRwuaXo5gHt4aYuIiD4a94xcSZcCbwb2lLSOZhTOOcBSSacA9wHHlcWXA0cBI8DjwMkAtjdK+hRwY1nuTNubHhyOiIgpNm7o2z5hM7MOHWNZA6du5n2WAEu2qrqIiGhVzsiNiKhIQj8ioiIJ/YiIiiT0IyIqktCPiKhIQj8ioiIJ/YiIiuR2iRHRqtwOcvuWPf2IiIok9CMiKpLQj4ioSEI/IqIiCf2IiIok9CMiKpLQj4ioSEI/IqIiCf2IiIok9CMiKpLQj4ioSEI/IqIiCf2IiIpMKvQl3SvpFkmrJK0sbXtIWiFpbfk7vbRL0mcljUhaLWn/NjYgIiImro09/bfYnmd7uDxfDFxjey5wTXkOcCQwtzwWAee1sO6IiNgKU9G9cwxwcZm+GDi2p/0SN64Ddpc0YwrWHxERmzHZ0DfwHUk3SVpU2vay/UCZ/gmwV5meCdzf89p1pS0iIvpksnfOOtj2ekm/BayQdEfvTNuW5K15w/LlsQhg7733nmR5ERHRa1J7+rbXl78PAd8E5gMPjnbblL8PlcXXA7N7Xj6rtG36nufbHrY9PDQ0NJnyIiJiE9sc+pJ+Q9KLRqeBw4FbgWXAwrLYQuDbZXoZcFIZxXMg8EhPN1BERPTBZLp39gK+KWn0ff7W9lWSbgSWSjoFuA84riy/HDgKGAEeB06exLojImIbbHPo274beOMY7T8FDh2j3cCp27q+iIiYvJyRGxFRkYR+RERFEvoRERWZ7Dj9iIhqzFl8RV/Xd+85R7f+ntnTj4ioSEI/IqIiCf2IiIok9CMiKpLQj4ioSEI/IqIiCf2IiIok9CMiKpLQj4ioSEI/IqIiCf2IiIok9CMiKpLQj4ioSEI/IqIiCf2IiIok9CMiKpLQj4ioSEI/IqIifQ99SQsk3SlpRNLifq8/IqJmfQ19SdOAzwFHAvsCJ0jat581RETUrN97+vOBEdt3234KuAw4ps81RERUS7b7tzLp3cAC239Unp8IHGD7tJ5lFgGLytNXA3f2rUDYE/iXPq6v37J9O7Zs346r39v2MttDY83YqY9FTIjt84Hzu1i3pJW2h7tYdz9k+3Zs2b4d1/a0bf3u3lkPzO55Pqu0RUREH/Q79G8E5kraR9IuwPHAsj7XEBFRrb5279h+WtJpwNXANGCJ7TX9rGEcnXQr9VG2b8eW7dtxbTfb1tcDuRER0a2ckRsRUZGEfkRERaoOfUlvk1T1f4OIqEvtgff7wFpJfynpNV0XM9UkTZf0hq7raIsas8dfMiJGVR36tt8L7AfcBVwk6R8lLZL0oo5La42k70l6saQ9gJuBCyR9puu62uBmFMLyruuYCpKmSbqj6zr6QdLLJP1emd5tUD5/kvaSdKGkK8vzfSWd0nVdVYc+gO1Hga/RXAdoBvAO4GZJf9ppYe15SdnGdwKX2D4A+L2Oa2rTzZLe1HURbbP9DHCnpL27rmUqSfpjms/fF0rTLOBb3VXUqotohqe/tDz/Z+BDnVVTVB36kt4u6ZvA94Cdgfm2jwTeCHyky9patJOkGcBxwOVdFzMFDgD+UdJdklZLukXS6q6Lasl0YI2kayQtG310XVTLTgUOAh4FsL0W+K1OK2rPnraXAv8GzXlKwDPdlrQdXnunz94FnGv7+72Nth/fHn6GteRMmr2NH9i+UdLLgbUd19SmI7ouYAp9ousC+uBJ209JAkDSTsCgnDz0C0m/SdkeSQcCj3RbUk7OQtJewGj3wA22H+qynth6kg4G5tr+kqQh4IW27+m6rhifpL8EfgacBPwp8F+A22z/904La4Gk/YG/AV4H3AoMAe+23ekv0apDX9J7gL+i6d4R8DvAn9n+Wpd1tal8qM4C/hW4CngD8GHb/7vTwloi6QxgGHi17VdJeinwd7YP6ri0SSt7hn8D/DtgF5pLl/zC9os7LaxFZcj0KcDhNJ/Bq4EvekCCqfxyeTXNtt1p+5cdl1R96P8YOGx0777sJX7X9hu7raw9klbZnifpHcBbgf8KfH9QtlHSKpoRWDfb3q+0rba9ww9NlbSS5qKEf0fzxXYS8Crbp3daWIskvRO4wvaTXdfStrJTeZXtxyR9HNgfOMv2zV3WVfWBXOB5m3Tn/JTB+28yetzmaJo94M77FFv2VNkrHO03/Y2O62mV7RFgmu1nbH8JWNB1TS17G/DPkr4s6a1lz3hQfKIE/sHAocCFwHkd1zRwAbe1rpJ0taT3SXofzZjvKzuuqW2Xl/Hevw1cU37NPNFxTW1aKukLwO5l+N93gQs6rqktj5dLkK8qJxB+mAH7zNo+GXglza+ZE4C7JH2x26paMzpS52jgAttX0HTTdarq7h149uflaP/vP9gelDHCzyonZj1i+5myJ/wi2z/puq62SDqMnj5h2ys6LqkVkl4GPEgTFB8GXgJ8vuz9DxRJO9P8ijkZ+I+29+y4pEmTdDnNTaIOo+na+VeawSKddq1WGfqSfmD7YEmP0XQLqGf2vwEbgU/b/nwnBbZI0gto+vH3tr1I0lyag56DOGZ/4Ejajebfrp/3iu4bSUfSXA7lzTQDKpYC3ylj2ndo5bO3ALjF9tpyvszrbX+n07pqDP3xlLG1P7L96q5rmSxJXwVuAk6y/bryP+KPbM/ruLRW9Hxx93oEWAl8xPbd/a+qHZLeRjO6bBfb+0iaB5xp++0dl9YaSZcCXwWuHJSDuZJebPvR8gv7OWxv7HdNvRL6myFphu0Huq5jskZvyCzpn3pGt/y465+YbZH0KWAd8Lc0v9iOB15Bc52h99t+c3fVTY6km4BDgO/1/NvdYvv13VbWrkE7V0bS5bbfKukentuTYNsv76g0YMAOCrVpEAK/eKp0EYyObnkFMBB7VMXbbX/B9mO2H7V9PnCE7a/SXMZgR/bLMUZbDdReWhnWeAPwHppLhVwv6d3dVjU5JfAF/K7tl9vep+fRaeBDLsNQgzNoTsqaLekrNAet39dpRe16XNJxNBftAng3vxqdtKMH5BpJ/wmYVo7FfAD4Ucc1te3jwJs2PVeGX/177pBsW9IVwHb3qyx7+gOujGR5J03QXwoM2/5elzW17A+AE4GHaEa6nAi8t/y6Oa3LwraVpC+XybuA19L8MruU5qJknV+lsWWDfK7MdnkF2PTpV0DSTOBl9Pyy2/Qic7H9kHQbzeWvrwTesun8rg8EtknSp2kuDXJpafp9YLXtj3ZXVTvK+TGvBO4DfkHTt++uzxZP6A84SX9B80FaQ7nEK83/eAMxAqR0B/wxMIdf/1L7w65qmixJHwDeD7ycZpz3s7PYDg4Etk3Su/j1c2W+2WU9bSnnWTyH7fv6XUuvhP6Ak3Qn8IZBGQ63KUk/Av6BZljqs9cqt/31zopqiaTzbL+/6zpi25UrbR5Mc3zph11fdwcS+gOv3KrtPbZ/3nUtU2H0gnJd1xFbZzPnV8Cvfs3s8FcSlfQ/aEYlfaM0HUtz/auzuqsqoT/wJH2d5k5g19AzVNP2BzorqkWSzqI52Wwg75UbO67yK/uNtp8oz3cDVnV90meGbA6+ZeUxqD4IfEzSk8AvGaA9xdjh/T/g+fxqCPGu/Poxmk5kTz92eOV097k0HzAAbP/f7iqKAEnfojnTeAVNV9ZhNCeirYPufm0n9AeUpFvYwslJXQ8ba4ukP6LZ258FrAIOpOnuObTTwqJ6khZuab7ti/tVS6907wyut5a/p5a/oyf8vJcd/0zVXh+k2Zu6zvZbJL0G+J8d1xSVkzQNONz2H3Rdy6YS+gNqdCywpMNGL9ZVfFTSzcDibipr3RO2n5CEpF1t3yFph786auzYyr0rXiZpF9tPdV1Pr4T+4JOkg2z/sDz5DwzOae4A6yTtDnwLWCHpYZozICO6djfwQ0nLaM7IBcD2Z7orKX36A0/SbwNLaO66JOBh4A+3h5NE2ibpd2m286rtbe8q6iPpjLHabX+y37X0SuhXQtJLAAbwxugRsRUS+hWQdDTN1Rp7hzSe2V1FEYNP0rWMMWjC9iEdlPOs9OkPOEn/C3gBzdUav0hzvfkbOi0qog7/rWf6+cC7gM7v/Zs9/QEnabXtN/T8fSHN/Uh/p+vaImoj6Qbb87usIXv6g2/0FPDHJb0U2AjM6LCeiCpscmP05wHDNAMNOpXQH3z/pwxp/DTNzcINXNBtSRFVuIlf3Rj9l8C9wCldFgSDNV47xnYH8Ey5vvzngOtoxrRHxNT6KDDP9j40Z8T/Ani825IS+jX4hO3HJB0MHEJzMPe8jmuKqMHHbT+6vX32EvqDb/RuUkcDF9i+Atilw3oiarFdfvYS+oNvvaQv0Nwnd7mkXcm/e0Q/bJefvQzZHHCSXgAsAG6xvVbSDOD1tr/TcWkRA217/ewl9CMiKtL5T42IiOifhH5EREUS+hERFUnoR0RUJKEfEVGR/w8dT8Wu42V8ZgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYxc8fx_H3ad"
      },
      "source": [
        "Data has been preprocessed already, using technique from this paper: https://www.aclweb.org/anthology/D18-1404/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYKK7ujRHfRt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "7a8b9667-fc0e-4566-a74d-1f5b0e00348b"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>emotions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>111247</th>\n",
              "      <td>i think about the difficult situations and rel...</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16170</th>\n",
              "      <td>i feel like im being assaulted by green</td>\n",
              "      <td>fear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42882</th>\n",
              "      <td>i skirt at the office feel dangerous</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92968</th>\n",
              "      <td>i suffer from emotional eating and i have been...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31148</th>\n",
              "      <td>i found myself feeling frustrated</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text emotions\n",
              "111247  i think about the difficult situations and rel...      joy\n",
              "16170             i feel like im being assaulted by green     fear\n",
              "42882                i skirt at the office feel dangerous    anger\n",
              "92968   i suffer from emotional eating and i have been...  sadness\n",
              "31148                   i found myself feeling frustrated    anger"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXovcl56NFPp"
      },
      "source": [
        "## reset index\n",
        "data.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSzoz9InH0Ta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6900b7bc-1bd3-47ac-c8f5-4b5351c9db9f"
      },
      "source": [
        "## check unique emotions in the dataset\n",
        "data.emotions.unique()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['joy', 'fear', 'anger', 'sadness', 'love', 'surprise'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJm31gKShQus"
      },
      "source": [
        "## Split the data and store into individual text files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ooNxSnPiztL"
      },
      "source": [
        "## uncomment the code below to generate the text files for your train, val, and test datasets.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_train, input_val, train_labels, val_labels = train_test_split(data.text.to_list(), \n",
        "                                                                    data.emotions.map(label2int).to_numpy(), \n",
        "                                                                    # data.emotions.to_numpy(), \n",
        "                                                                    test_size=0.2)\n",
        "\n",
        "# Split the validataion further to obtain a holdout dataset (for testing) -- split 50:50\n",
        "input_val, input_test, val_labels, test_labels = train_test_split(input_val, val_labels, test_size=0.5)\n",
        "\n",
        "\n",
        "## create a dataframe for each dataset\n",
        "# train_dataset = pd.DataFrame(data={\"text\": input_train, \"class\": train_labels})\n",
        "# val_dataset = pd.DataFrame(data={\"text\": input_val, \"class\": val_labels})\n",
        "# test_dataset = pd.DataFrame(data={\"text\": input_test, \"class\": test_labels})\n",
        "# final_dataset = {\"train\": train_dataset, \"val\": val_dataset , \"test\": test_dataset }\n",
        "\n",
        "# train_dataset.to_csv(train_path, sep=\";\",header=False, index=False)\n",
        "# val_dataset.to_csv(test_path, sep=\";\",header=False, index=False)\n",
        "# test_dataset.to_csv(val_path, sep=\";\",header=False, index=False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CChH5iO60uHH"
      },
      "source": [
        "## Preparing the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE4WI6DL0uHI"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Define the model repo\n",
        "model_name = \"distilroberta-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8Jrawcr5lpN"
      },
      "source": [
        "train_encodings = tokenizer(input_train, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(input_val, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(input_test, truncation=True, padding=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeG8TtJX6HLV"
      },
      "source": [
        "import torch \n",
        "class EmoDataset(torch.utils.data.Dataset):\n",
        "    # def __init__(self, path):\n",
        "        # super().__init__()\n",
        "        # self.data_column = \"text\"\n",
        "        # self.class_column = \"class\"\n",
        "        # self.data = pd.read_csv(path, sep=\";\", header=None, \n",
        "        #                         names=[self.data_column, self.class_column],\n",
        "        #                         engine=\"python\")\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "    # def __getitem__(self, idx):\n",
        "    #     return self.data.loc[idx, self.data_column], label2int[self.data.loc[idx, self.class_column]]\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    # def __len__(self):\n",
        "    #     return self.data.shape[0]\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBfjRqJX6oyp"
      },
      "source": [
        "train_dataset = EmoDataset(train_encodings, train_labels)\n",
        "val_dataset = EmoDataset(val_encodings, val_labels)\n",
        "test_dataset = EmoDataset(test_encodings, test_labels)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RFifOoY7Hsc"
      },
      "source": [
        "# Building Custom Classification head on top of LM base model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSUMm4Oq7nvR"
      },
      "source": [
        "Use Mish activiation function as in the one proposed in the original tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCEDXLxq628O"
      },
      "source": [
        "# # from https://github.com/digantamisra98/Mish/blob/b5f006660ac0b4c46e2c6958ad0301d7f9c59651/Mish/Torch/mish.py\n",
        "# import torch.nn.functional as F\n",
        "# from torch import nn\n",
        "# @torch.jit.script\n",
        "# def mish(input):\n",
        "#     return input * torch.tanh(F.softplus(input))\n",
        "  \n",
        "# class Mish(nn.Module):\n",
        "#     def forward(self, input):\n",
        "#         return mish(input)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6Ln6KWm74ku"
      },
      "source": [
        "The model we will use to do the fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VDRSRsc71H2"
      },
      "source": [
        "# class EmoModel(nn.Module):\n",
        "#     def __init__(self, base_model, n_classes, \n",
        "#                  base_model_output_size=768, dropout=0.05):\n",
        "#         super().__init__()\n",
        "#         self.base_model = base_model\n",
        "        \n",
        "#         self.classifier = nn.Sequential(\n",
        "#             nn.Dropout(dropout),\n",
        "#             nn.Linear(base_model_output_size, base_model_output_size),\n",
        "#             Mish(),\n",
        "#             nn.Dropout(dropout),\n",
        "#             # nn.Linear(base_model_output_size, n_classes)\n",
        "#             nn.Softmax(n_classes)\n",
        "#         )\n",
        "        \n",
        "#         for layer in self.classifier:\n",
        "#             if isinstance(layer, nn.Linear):\n",
        "#                 layer.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#                 if layer.bias is not None:\n",
        "#                     layer.bias.data.zero_()\n",
        "\n",
        "#     def forward(self, input_ids, attention_mask, *args):\n",
        "#         # print(\"In Emo\")\n",
        "#         # print(\"Inputs\", type(inputs), len(inputs))\n",
        "#         # print(\"Inputs0\", type(inputs[0]), \"Inputs1\", type(inputs[1]))\n",
        "#         # input_ids, attention_mask = inputs#['input_ids'], inputs['attention_mask']\n",
        "#         hidden_states = self.base_model(input_ids=input_ids, \n",
        "#                                         attention_mask=attention_mask)\n",
        "#         # maybe do some pooling / RNNs... go crazy here!\n",
        "        \n",
        "#         # use the <s> representation\n",
        "#         return self.classifier(hidden_states[0][:, 0, :])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C4lCWyyWC8B",
        "outputId": "11145974-8e50-45f5-b97c-e430c4fd1871"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "\n",
        "del tokenizer\n",
        "# del model\n",
        "# del trainer\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM5az7It0uHK",
        "outputId": "1664a3c6-c00e-4085-a78a-d6f7fd615873"
      },
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, \n",
        "                                                           num_labels=len(label2int))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoyYHeGHQXj"
      },
      "source": [
        "# emotion_model = EmoModel(base_model=model.base_model, \n",
        "#                          n_classes=len(label2int)) "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q29zPQU_0uHK"
      },
      "source": [
        "# Fine-tuning in PyTorch with the Trainer API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu_R5bLS0uHK"
      },
      "source": [
        "Since PyTorch does not provide a training loop, the 🤗 Transformers library provides a `Trainer`\n",
        "API that is optimized for 🤗 Transformers models, with a wide range of training options and with built-in features like\n",
        "logging, gradient accumulation, and mixed precision.\n",
        "\n",
        "First, let's define our model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbOfFrLR0uHK"
      },
      "source": [
        "This will issue a warning about some of the pretrained weights not being used and some weights being randomly\n",
        "initialized. That's because we are throwing away the pretraining head of the BERT model to replace it with a\n",
        "classification head which is randomly initialized. We will fine-tune this model on our task, transferring the knowledge\n",
        "of the pretrained model to it (which is why doing this is called transfer learning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7vqv_Pyii7E"
      },
      "source": [
        "# from transformers import TrainerCallback\n",
        "\n",
        "# class PrinterCallback(TrainerCallback):\n",
        "\n",
        "#     def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "#         _ = logs.pop(\"total_flos\", None)\n",
        "#         if state.is_local_process_zero:\n",
        "#             print(logs)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPNh8DxL0uHM"
      },
      "source": [
        "which will start a training that you can follow with a progress bar, which should take a couple of minutes to complete\n",
        "(as long as you hav access to a GPU). It won't actually tell you anything useful about how well (or badly) your model\n",
        "is performing however as by default, there is no evaluation during training, and we didn't tell the\n",
        "`Trainer` to compute any metrics. Let's have a look on how to do that now!\n",
        "\n",
        "To have the `Trainer` compute and report metrics, we need to give it a `compute_metrics`\n",
        "function that takes predictions and labels (grouped in a namedtuple called `EvalPrediction`) and\n",
        "return a dictionary with string items (the metric names) and float values (the metric values).\n",
        "\n",
        "The 🤗 Datasets library provides an easy way to get the common metrics used in NLP with the `load_metric` function.\n",
        "here we simply use accuracy. Then we define the `compute_metrics` function that just convert logits to predictions\n",
        "(remember that all 🤗 Transformers models return the logits) and feed them to `compute` method of this metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD-Hp2-N0uHM"
      },
      "source": [
        "The compute function needs to receive a tuple (with logits and labels) and has to return a dictionary with string keys\n",
        "(the name of the metric) and float values. It will be called at the end of each evaluation phase on the whole arrays of\n",
        "predictions/labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rrszPR72r1U"
      },
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    metrics_dict = metric.compute(predictions=predictions, references=labels)\n",
        "    # metrics_dict.update({'roc_auc': roc_auc_score(labels, logits, \n",
        "                                                #   average='micro', multi_class='ovr')})\n",
        "    return metrics_dict"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x94gzCkvAOeR"
      },
      "source": [
        "Then, to define our `Trainer`, we will need to instantiate a\n",
        "`TrainingArguments`. This class contains all the hyperparameters we can tune for the\n",
        "`Trainer` or the flags to activate the different training options it supports. Let's begin by\n",
        "using all the defaults, the only thing we then have to provide is a directory in which the checkpoints will be saved:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp4f1q9q0uHL"
      },
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=5,              # total number of training epochs\n",
        "    per_device_train_batch_size=64,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    # model=emotion_model,\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,            # evaluation dataset\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "hvq8aQVc7VXd",
        "outputId": "758c1a29-1850-4f26-a18b-95a71a66ea56"
      },
      "source": [
        "trainer_output = trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='189' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [189/625 01:35 < 03:42, 1.96 it/s, Epoch 1.50/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.496600</td>\n",
              "      <td>1.102911</td>\n",
              "      <td>0.571000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzfkFiij1rZm"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa_-hpBa6VNR"
      },
      "source": [
        "# from torch.utils.data import DataLoader, Dataset\n",
        "# class EmoDataset(Dataset):\n",
        "#     def __init__(self, path):\n",
        "#         super().__init__()\n",
        "#         self.data_column = \"text\"\n",
        "#         self.class_column = \"class\"\n",
        "#         self.data = pd.read_csv(path, sep=\";\", header=None, \n",
        "#                                 names=[self.data_column, self.class_column],\n",
        "#                                 engine=\"python\")\n",
        "#         print(path)\n",
        "#         display(self.data.head(2))\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         return self.data.loc[idx, self.data_column], label2int[self.data.loc[idx, self.class_column]]\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.data.shape[0]\n",
        "\n",
        "# import torch\n",
        "\n",
        "# class TokenizersCollateFn:\n",
        "#     def __init__(self, tokenizer):\n",
        "#         self.tokenizer = tokenizer\n",
        "\n",
        "#     def __call__(self, batch):\n",
        "#         # encoded = self.tokenizer.encode_batch([x[0] for x in batch])\n",
        "#         encoded = self.tokenizer([x[0] for x in batch], \n",
        "#                    max_length=150, padding=True, truncation=True, \n",
        "#                    return_tensors=\"pt\")\n",
        "        \n",
        "#         sequences_padded = torch.tensor([enc.ids for enc in encoded])\n",
        "#         attention_masks_padded = torch.tensor([enc.attention_mask for enc in encoded])\n",
        "#         labels = torch.tensor([x[1] for x in batch])\n",
        "        \n",
        "#         return (sequences_padded, attention_masks_padded), labels\n",
        "#         # return inputs, labels"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY0g7fwaM6m3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO1r-zbAM6Jv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WMyu4x4M5jy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzbrL-iN0uHQ"
      },
      "source": [
        "Note that if you are used to freezing the body of your pretrained model (like in computer vision) the above may seem a\n",
        "bit strange, as we are directly fine-tuning the whole model without taking any precaution. It actually works better\n",
        "this way for Transformers model (so this is not an oversight on our side). If you're not familiar with what \"freezing\n",
        "the body\" of the model means, forget you read this paragraph.\n",
        "\n",
        "Now to check the results, we need to write the evaluation loop. Like in the [trainer section](#trainer) we will\n",
        "use a metric from the datasets library. Here we accumulate the predictions at each batch before computing the final\n",
        "result when the loop is finished."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al6LOymV0uHQ"
      },
      "source": [
        "metric= load_metric(\"accuracy\")\n",
        "model.eval()\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}