{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLghoSjJ1xhf"
      },
      "source": [
        "# Install libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-jqprc70uHE"
      },
      "source": [
        "# %%capture\n",
        "# # Transformers installation\n",
        "# ! pip install transformers\n",
        "# ! pip install datasets\n",
        "# # To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# # ! pip install git+https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hu70Ng0Eqls"
      },
      "source": [
        "# Getting the Data and Preview it\n",
        "Below we are going to load the data and show you how to create the splits. However, we don't need to split the data manually becuase I have already created the splits and stored those files seperately which you can quickly download below:\n",
        "\n",
        "https://www.kaggle.com/praveengovi/emotions-dataset-for-nlp/code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ3SoJH3fUsq"
      },
      "source": [
        "# %%capture \n",
        "# !wget https://www.dropbox.com/s/ikkqxfdbdec3fuj/test.txt\n",
        "# !wget https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt\n",
        "# !wget https://www.dropbox.com/s/2mzialpsgf9k5l3/val.txt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4rYMiOuSLWM"
      },
      "source": [
        "# with open('train.txt') as f:\n",
        "#     print(f.read()[:1000])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_03fxufWX_G"
      },
      "source": [
        "## export the datasets as txt files\n",
        "## EXERCISE: Change this to an address\n",
        "\n",
        "# train_path = \"train.txt\"\n",
        "# test_path = \"test.txt\"\n",
        "# val_path = \"val.txt\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t23zHggkEpc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5960316-2d2d-40ec-c4ea-822f56a3c3cf"
      },
      "source": [
        "!wget https://www.dropbox.com/s/607ptdakxuh5i4s/merged_training.pkl"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-12 12:17:12--  https://www.dropbox.com/s/607ptdakxuh5i4s/merged_training.pkl\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.18, 2620:100:6030:18::a27d:5012\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/607ptdakxuh5i4s/merged_training.pkl [following]\n",
            "--2021-06-12 12:17:12--  https://www.dropbox.com/s/raw/607ptdakxuh5i4s/merged_training.pkl\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucd97fd84fb1015a2d7492c207cc.dl.dropboxusercontent.com/cd/0/inline/BQQt7IfUpc6_YZTMrrjSN5T5_mczFMebQo6MHoOxWdIzkCQvwbfjnk8AIhCc2nM5DnonNDJe-NJT4ttuTICwWOrNvCSyezZWcKC8UJnHsukR2v_srbQgafN6aF2Bx7XjXLMYPge_xd_QbYjKN6uw8EPJ/file# [following]\n",
            "--2021-06-12 12:17:13--  https://ucd97fd84fb1015a2d7492c207cc.dl.dropboxusercontent.com/cd/0/inline/BQQt7IfUpc6_YZTMrrjSN5T5_mczFMebQo6MHoOxWdIzkCQvwbfjnk8AIhCc2nM5DnonNDJe-NJT4ttuTICwWOrNvCSyezZWcKC8UJnHsukR2v_srbQgafN6aF2Bx7XjXLMYPge_xd_QbYjKN6uw8EPJ/file\n",
            "Resolving ucd97fd84fb1015a2d7492c207cc.dl.dropboxusercontent.com (ucd97fd84fb1015a2d7492c207cc.dl.dropboxusercontent.com)... 162.125.82.15, 2620:100:6030:15::a27d:500f\n",
            "Connecting to ucd97fd84fb1015a2d7492c207cc.dl.dropboxusercontent.com (ucd97fd84fb1015a2d7492c207cc.dl.dropboxusercontent.com)|162.125.82.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BQRjejdjWLaAnH0yJZjNjfOBAN0smSoRWshtaOoW1pV6RVXn-VswK484om6eQ5AI5bLngxrtovUBIhLsKIJJbZgnUvp8M42DQGOCIMPBZrLBamX_tPuTqJKNyo1B8yHpFpaGM6ZXzGpORJYvTF83lKefGjW0TrHLih-bkZnhFj_5Pk4I7EnzNcrQFUd_zUejvt60PlsHv1jvf40YqgpMePIlYaUID5gVNI0SYTplbcxFb8ba1BANUtu7GzkTStjKnTLUd6E8AcuyH4ZvETcO2t5C5UT2vxv0VblW5ofG15pY6rhjt_KA3N48bexJUk-zlYwZCLIhnWhQ4ZJfEbhg09aly8WtVBzInaw6jT6cabgeqbN_rZZo4_OB-1LpjzYWZbA/file [following]\n",
            "--2021-06-12 12:17:14--  https://ucd97fd84fb1015a2d7492c207cc.dl.dropboxusercontent.com/cd/0/inline2/BQRjejdjWLaAnH0yJZjNjfOBAN0smSoRWshtaOoW1pV6RVXn-VswK484om6eQ5AI5bLngxrtovUBIhLsKIJJbZgnUvp8M42DQGOCIMPBZrLBamX_tPuTqJKNyo1B8yHpFpaGM6ZXzGpORJYvTF83lKefGjW0TrHLih-bkZnhFj_5Pk4I7EnzNcrQFUd_zUejvt60PlsHv1jvf40YqgpMePIlYaUID5gVNI0SYTplbcxFb8ba1BANUtu7GzkTStjKnTLUd6E8AcuyH4ZvETcO2t5C5UT2vxv0VblW5ofG15pY6rhjt_KA3N48bexJUk-zlYwZCLIhnWhQ4ZJfEbhg09aly8WtVBzInaw6jT6cabgeqbN_rZZo4_OB-1LpjzYWZbA/file\n",
            "Reusing existing connection to ucd97fd84fb1015a2d7492c207cc.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49991846 (48M) [application/octet-stream]\n",
            "Saving to: ‘merged_training.pkl.10’\n",
            "\n",
            "merged_training.pkl 100%[===================>]  47.68M  13.1MB/s    in 3.6s    \n",
            "\n",
            "2021-06-12 12:17:18 (13.1 MB/s) - ‘merged_training.pkl.10’ saved [49991846/49991846]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQrMSUTRF06B"
      },
      "source": [
        "import pickle\n",
        "\n",
        "## helper function\n",
        "def load_from_pickle(directory):\n",
        "    return pickle.load(open(directory,\"rb\"))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGz89mNSHaYM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "0f3e0e5a-5eab-47aa-9cc2-0f3292f214ed"
      },
      "source": [
        "data = load_from_pickle(directory=\"merged_training.pkl\")\n",
        "\n",
        "## emotion labels\n",
        "label2int = {\n",
        "  \"sadness\": 0,\n",
        "  \"joy\": 1,\n",
        "  \"love\": 2,\n",
        "  \"anger\": 3,\n",
        "  \"fear\": 4,\n",
        "  \"surprise\": 5\n",
        "}\n",
        "\n",
        "data = data[data[\"emotions\"].isin(label2int.keys())]\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "data = data.sample(n=1000);\n",
        "\n",
        "data.emotions.value_counts().plot.bar()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(416809, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f88ffbcccd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYxc8fx_H3ad"
      },
      "source": [
        "Data has been preprocessed already, using technique from this paper: https://www.aclweb.org/anthology/D18-1404/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYKK7ujRHfRt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f033e37d-32ad-4c52-b0a9-363ce40ceb05"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>emotions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16013</th>\n",
              "      <td>i feel disgusted at him and at myself for havi...</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44023</th>\n",
              "      <td>i feel like a pest when i call the doctor s of...</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75928</th>\n",
              "      <td>i just have the feeling that for some people i...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>618</th>\n",
              "      <td>the loss of a person i loved very much</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58149</th>\n",
              "      <td>i feel quite lucky</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text emotions\n",
              "16013  i feel disgusted at him and at myself for havi...    anger\n",
              "44023  i feel like a pest when i call the doctor s of...      joy\n",
              "75928  i just have the feeling that for some people i...  sadness\n",
              "618               the loss of a person i loved very much  sadness\n",
              "58149                                 i feel quite lucky      joy"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXovcl56NFPp"
      },
      "source": [
        "## reset index\n",
        "data.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSzoz9InH0Ta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a9e179-ddee-44c2-eedc-db30d7d9d062"
      },
      "source": [
        "## check unique emotions in the dataset\n",
        "data.emotions.unique()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['anger', 'joy', 'sadness', 'surprise', 'love', 'fear'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJm31gKShQus"
      },
      "source": [
        "## Split the data and store into individual text files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv9TyIZeZfQ2"
      },
      "source": [
        "# dummies = pd.get_dummies(data.emotions)\n",
        "# ohe_mapping = {i: c for i, c in enumerate(dummies.columns)}"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ooNxSnPiztL"
      },
      "source": [
        "## uncomment the code below to generate the text files for your train, val, and test datasets.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_train, input_val, train_labels, val_labels = train_test_split(data.text.to_list(), \n",
        "                                                                    # dummies.values,\n",
        "                                                                    data.emotions.map(label2int).to_numpy(), \n",
        "                                                                    # data.emotions.to_numpy(), \n",
        "                                                                    test_size=0.2)\n",
        "\n",
        "# Split the validataion further to obtain a holdout dataset (for testing) -- split 50:50\n",
        "input_val, input_test, val_labels, test_labels = train_test_split(input_val, val_labels, test_size=0.5)\n",
        "\n",
        "\n",
        "## create a dataframe for each dataset\n",
        "# train_dataset = pd.DataFrame(data={\"text\": input_train, \"class\": train_labels})\n",
        "# val_dataset = pd.DataFrame(data={\"text\": input_val, \"class\": val_labels})\n",
        "# test_dataset = pd.DataFrame(data={\"text\": input_test, \"class\": test_labels})\n",
        "# final_dataset = {\"train\": train_dataset, \"val\": val_dataset , \"test\": test_dataset }\n",
        "\n",
        "# train_dataset.to_csv(train_path, sep=\";\",header=False, index=False)\n",
        "# val_dataset.to_csv(test_path, sep=\";\",header=False, index=False)\n",
        "# test_dataset.to_csv(val_path, sep=\";\",header=False, index=False)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CChH5iO60uHH"
      },
      "source": [
        "## Preparing the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE4WI6DL0uHI"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Define the model repo\n",
        "model_name = \"distilroberta-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8Jrawcr5lpN"
      },
      "source": [
        "train_encodings = tokenizer(input_train, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(input_val, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(input_test, truncation=True, padding=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeG8TtJX6HLV"
      },
      "source": [
        "import torch \n",
        "class EmoDataset(torch.utils.data.Dataset):\n",
        "    # def __init__(self, path):\n",
        "        # super().__init__()\n",
        "        # self.data_column = \"text\"\n",
        "        # self.class_column = \"class\"\n",
        "        # self.data = pd.read_csv(path, sep=\";\", header=None, \n",
        "        #                         names=[self.data_column, self.class_column],\n",
        "        #                         engine=\"python\")\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "    # def __getitem__(self, idx):\n",
        "    #     return self.data.loc[idx, self.data_column], label2int[self.data.loc[idx, self.class_column]]\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        # labels = torch.tensor(self.labels[idx])\n",
        "        return item#, labels\n",
        "\n",
        "    # def __len__(self):\n",
        "    #     return self.data.shape[0]\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBfjRqJX6oyp"
      },
      "source": [
        "train_dataset = EmoDataset(train_encodings, train_labels)\n",
        "val_dataset = EmoDataset(val_encodings, val_labels)\n",
        "test_dataset = EmoDataset(test_encodings, test_labels)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RFifOoY7Hsc"
      },
      "source": [
        "# Building Custom Classification head on top of LM base model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSUMm4Oq7nvR"
      },
      "source": [
        "Use Mish activiation function as in the one proposed in the original tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VDRSRsc71H2"
      },
      "source": [
        "# Use Mish activiation function as in the one proposed in the original tutorial\n",
        "# from https://github.com/digantamisra98/Mish/blob/b5f006660ac0b4c46e2c6958ad0301d7f9c59651/Mish/Torch/mish.py\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "@torch.jit.script\n",
        "def mish(input):\n",
        "    return input * torch.tanh(F.softplus(input))\n",
        "  \n",
        "class Mish(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return mish(input)\n",
        "\n",
        "class EmoModel(nn.Module):\n",
        "    def __init__(self, base_model, n_classes, \n",
        "                 base_model_output_size=768, dropout=0.05):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.n_classes = n_classes\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(base_model_output_size, base_model_output_size),\n",
        "            Mish(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(in_features=base_model_output_size, \n",
        "                      out_features=n_classes),\n",
        "            nn.Softmax(dim=-1) \n",
        "        )\n",
        "        \n",
        "        for layer in self.classifier:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                layer.weight.data.normal_(mean=0.0, std=0.02)\n",
        "                if layer.bias is not None:\n",
        "                    layer.bias.data.zero_()\n",
        "\n",
        "    def forward(self, inputs, *args):\n",
        "        # print(\"In Emo\")\n",
        "        # print(\"Inputs\", type(inputs), len(inputs))\n",
        "        # print(\"Inputs0\", type(inputs[0]), \"Inputs1\", type(inputs[1]))\n",
        "        # print('inputs', inputs)\n",
        "        input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']\n",
        "        hidden_states = self.base_model(input_ids=input_ids, \n",
        "                                        attention_mask=attention_mask)\n",
        "        # maybe do some pooling / RNNs... go crazy here!\n",
        "        \n",
        "        # use the <s> representation\n",
        "        return self.classifier(hidden_states[0][:, 0, :])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C4lCWyyWC8B"
      },
      "source": [
        "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# print(device)\n",
        "\n",
        "# del tokenizer\n",
        "# # del model\n",
        "# # del trainer\n",
        "# torch.cuda.empty_cache()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM5az7It0uHK",
        "outputId": "26f306d9-30e2-463c-e359-c2182fe4c651"
      },
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, \n",
        "                                                           num_labels=len(label2int))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoyYHeGHQXj",
        "outputId": "7dd90ddf-e78d-45e4-d580-f07050fc561e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "emotion_model = EmoModel(base_model=model.base_model, \n",
        "                         n_classes=len(label2int)) \n",
        "\n",
        "output = emotion_model(val_dataset.__getitem__(slice(2)))\n",
        "output"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1774, 0.1588, 0.1634, 0.1802, 0.1801, 0.1400],\n",
              "        [0.1867, 0.1624, 0.1624, 0.1776, 0.1761, 0.1347]],\n",
              "       grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk4nCtCt2lTq",
        "outputId": "66b6bea5-bbaf-41b4-a754-b5851b2b70b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "val_dataset.__getitem__(slice(2))['labels']"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPq-Kf8S1-RM",
        "outputId": "5685980d-d5dc-4a3f-8892-5125775d10a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "loss_fct = torch.nn.NLLLoss()\n",
        "loss_fct(torch.log(output), val_dataset.__getitem__(slice(2))['labels'])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.7329, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y-HvuchIbx0",
        "outputId": "65034e1e-070c-4beb-8963-f24b1a06339f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(val_dataset.__getitem__(slice(2))['input_ids'])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q29zPQU_0uHK"
      },
      "source": [
        "# Fine-tuning in PyTorch with the Trainer API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu_R5bLS0uHK"
      },
      "source": [
        "Since PyTorch does not provide a training loop, the 🤗 Transformers library provides a `Trainer`\n",
        "API that is optimized for 🤗 Transformers models, with a wide range of training options and with built-in features like\n",
        "logging, gradient accumulation, and mixed precision.\n",
        "\n",
        "First, let's define our model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbOfFrLR0uHK"
      },
      "source": [
        "This will issue a warning about some of the pretrained weights not being used and some weights being randomly\n",
        "initialized. That's because we are throwing away the pretraining head of the BERT model to replace it with a\n",
        "classification head which is randomly initialized. We will fine-tune this model on our task, transferring the knowledge\n",
        "of the pretrained model to it (which is why doing this is called transfer learning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7vqv_Pyii7E"
      },
      "source": [
        "# from transformers import TrainerCallback\n",
        "\n",
        "# class PrinterCallback(TrainerCallback):\n",
        "\n",
        "#     def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "#         _ = logs.pop(\"total_flos\", None)\n",
        "#         if state.is_local_process_zero:\n",
        "#             print(logs)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPNh8DxL0uHM"
      },
      "source": [
        "which will start a training that you can follow with a progress bar, which should take a couple of minutes to complete\n",
        "(as long as you hav access to a GPU). It won't actually tell you anything useful about how well (or badly) your model\n",
        "is performing however as by default, there is no evaluation during training, and we didn't tell the\n",
        "`Trainer` to compute any metrics. Let's have a look on how to do that now!\n",
        "\n",
        "To have the `Trainer` compute and report metrics, we need to give it a `compute_metrics`\n",
        "function that takes predictions and labels (grouped in a namedtuple called `EvalPrediction`) and\n",
        "return a dictionary with string items (the metric names) and float values (the metric values).\n",
        "\n",
        "The 🤗 Datasets library provides an easy way to get the common metrics used in NLP with the `load_metric` function.\n",
        "here we simply use accuracy. Then we define the `compute_metrics` function that just convert logits to predictions\n",
        "(remember that all 🤗 Transformers models return the logits) and feed them to `compute` method of this metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD-Hp2-N0uHM"
      },
      "source": [
        "The compute function needs to receive a tuple (with logits and labels) and has to return a dictionary with string keys\n",
        "(the name of the metric) and float values. It will be called at the end of each evaluation phase on the whole arrays of\n",
        "predictions/labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rrszPR72r1U"
      },
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    metrics_dict = dict()\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    print('in metrics. pred', len(predictions), 'logits', len(logits), 'labels', len(labels))\n",
        "    metrics_dict.update(metric.compute(predictions=predictions, references=labels))\n",
        "    # metrics_dict.update({'roc_auc': roc_auc_score(labels, logits, \n",
        "    #                                               average='micro', multi_class='ovr')})\n",
        "    return metrics_dict"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x94gzCkvAOeR"
      },
      "source": [
        "Then, to define our `Trainer`, we will need to instantiate a\n",
        "`TrainingArguments`. This class contains all the hyperparameters we can tune for the\n",
        "`Trainer` or the flags to activate the different training options it supports. Let's begin by\n",
        "using all the defaults, the only thing we then have to provide is a directory in which the checkpoints will be saved:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp4f1q9q0uHL"
      },
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=5,              # total number of training epochs\n",
        "    per_device_train_batch_size=64,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# class MultilabelTrainer(Trainer):\n",
        "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
        "#         labels = inputs.pop(\"labels\")\n",
        "#         outputs = model(**inputs)\n",
        "#         logits = outputs.logits\n",
        "#         loss_fct = torch.nn.BCEWithLogitsLoss()\n",
        "#         loss = loss_fct(logits.view(-1, self.model.config.num_labels),\n",
        "#                         labels.float().view(-1, self.model.config.num_labels))\n",
        "#         return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "class MultiClassTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(inputs)\n",
        "        loss_fct = torch.nn.NLLLoss()\n",
        "        # loss = loss_fct(outputs.view(-1, self.model.n_classes),\n",
        "        #                 labels.float().view(-1, self.model.n_classes))\n",
        "        loss = loss_fct(outputs, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "trainer = MultiClassTrainer(\n",
        "    # model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    model=emotion_model,\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,            # evaluation dataset\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvq8aQVc7VXd"
      },
      "source": [
        "trainer_output = trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzfkFiij1rZm",
        "outputId": "4a1a24df-a25d-4a0e-fee3-77eac168ba8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "trainer.evaluate(test_dataset)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs len 64\n",
            "inputs attention_mask 64\n",
            "inputs labels 64\n",
            "lens obtained for batch. logits, labels 63 64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "inputs len 36\n",
            "inputs attention_mask 36\n",
            "inputs labels 36\n",
            "lens obtained for batch. logits, labels 35 36\n",
            "num_samples in trainer 100\n",
            "all_preds before 98\n",
            "all_preds after 98\n",
            "all_labels before 100\n",
            "all_labels after 100\n",
            "in metrics. pred 98 logits 98 labels 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/metric.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, predictions, references)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrowInvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mtyped_sequence_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyped_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mpa_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pydict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyped_sequence_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_pydict\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mArrowInvalid\u001b[0m: Column 1 named references expected length 98 but got length 100",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-720de917bfdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2006\u001b[0m             \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m             \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m             \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         )\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2228\u001b[0m         \u001b[0;31m# Metrics!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall_preds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall_labels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvalPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2231\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-a58e36792dea>\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_pred)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'in metrics. pred'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logits'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'labels'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmetrics_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m# metrics_dict.update({'roc_auc': roc_auc_score(labels, logits,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#                                               average='micro', multi_class='ovr')})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/metric.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/metric.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, predictions, references)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrowInvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             raise ValueError(\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0;34mf\"Predictions and/or references don't match the expected format.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m                 \u001b[0;34mf\"Expected format: {self.features},\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;34mf\"Input predictions: {predictions},\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Predictions and/or references don't match the expected format.\nExpected format: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)},\nInput predictions: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3\n 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3],\nInput references: [0 1 1 0 0 5 1 1 1 5 5 1 5 2 1 2 2 1 3 0 0 2 0 0 3 0 2 0 1 1 1 3 5 2 1 2 3\n 1 1 2 3 4 0 0 0 1 3 0 1 3 0 0 0 2 0 3 1 4 4 3 0 3 0 0 0 5 0 0 4 1 1 2 1 0\n 3 5 3 0 1 0 1 2 1 1 1 3 2 4 3 1 1 1 3 0 4 1 0 4 0 1]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa_-hpBa6VNR"
      },
      "source": [
        "# from torch.utils.data import DataLoader, Dataset\n",
        "# class EmoDataset(Dataset):\n",
        "#     def __init__(self, path):\n",
        "#         super().__init__()\n",
        "#         self.data_column = \"text\"\n",
        "#         self.class_column = \"class\"\n",
        "#         self.data = pd.read_csv(path, sep=\";\", header=None, \n",
        "#                                 names=[self.data_column, self.class_column],\n",
        "#                                 engine=\"python\")\n",
        "#         print(path)\n",
        "#         display(self.data.head(2))\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         return self.data.loc[idx, self.data_column], label2int[self.data.loc[idx, self.class_column]]\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.data.shape[0]\n",
        "\n",
        "# import torch\n",
        "\n",
        "# class TokenizersCollateFn:\n",
        "#     def __init__(self, tokenizer):\n",
        "#         self.tokenizer = tokenizer\n",
        "\n",
        "#     def __call__(self, batch):\n",
        "#         # encoded = self.tokenizer.encode_batch([x[0] for x in batch])\n",
        "#         encoded = self.tokenizer([x[0] for x in batch], \n",
        "#                    max_length=150, padding=True, truncation=True, \n",
        "#                    return_tensors=\"pt\")\n",
        "        \n",
        "#         sequences_padded = torch.tensor([enc.ids for enc in encoded])\n",
        "#         attention_masks_padded = torch.tensor([enc.attention_mask for enc in encoded])\n",
        "#         labels = torch.tensor([x[1] for x in batch])\n",
        "        \n",
        "#         return (sequences_padded, attention_masks_padded), labels\n",
        "#         # return inputs, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY0g7fwaM6m3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO1r-zbAM6Jv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WMyu4x4M5jy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzbrL-iN0uHQ"
      },
      "source": [
        "Note that if you are used to freezing the body of your pretrained model (like in computer vision) the above may seem a\n",
        "bit strange, as we are directly fine-tuning the whole model without taking any precaution. It actually works better\n",
        "this way for Transformers model (so this is not an oversight on our side). If you're not familiar with what \"freezing\n",
        "the body\" of the model means, forget you read this paragraph.\n",
        "\n",
        "Now to check the results, we need to write the evaluation loop. Like in the [trainer section](#trainer) we will\n",
        "use a metric from the datasets library. Here we accumulate the predictions at each batch before computing the final\n",
        "result when the loop is finished."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al6LOymV0uHQ"
      },
      "source": [
        "metric= load_metric(\"accuracy\")\n",
        "model.eval()\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}